meta:
  mkdocs:
    destination:
      - "validations/submissions.md"
  name: Submitting new permutations
  description: |
    Process for submitting new validation permutations
  long-description: |
    ## Repo file locations

    Typically, a new permutation will require a Jenkins Job Builder
    specification, an OGC Spec file, and any integration testing required.


    ### Jenkins Job Builder (validate.yaml)

    For validation tests such as these, a Jenkins Job Builder file already exists in the github repo:

    [validate.yaml](https://github.com/charmed-kubernetes/jenkins/blob/master/jobs/validate.yaml)

    To add a new permutation (aka variation, aka addons) a new **job** stanza
    needs to be added. You can use any of the existing ones listed as a base,
    for example, if you were creating a permutation that would test some new
    logging component, you could copy and paste the below job to the existing
    **validate.yaml** file:

    ```yaml

    - job:
        name: 'validate-ck-logging'
        description: |
          Validates CK, with Logging.
        node: runner-cloud
        project-type: freestyle
        scm:
          - k8s-jenkins-jenkaas
        wrappers:
          - default-job-wrapper
          - ci-creds
        properties:
          - build-discarder:
              num-to-keep: 7
        triggers:
            - timed: "@daily"
        builders:
          - set-env:
              JOB_SPEC_DIR: "jobs/validate"
          - ogc-execute-spec:
              JOB_SPEC_DIR: "jobs/validate"
              JOB_SPEC_FILE: "logging-spec.yml"
    ```

    The most important parts to have set is the **builders** section as this is
    where you will tell the Jenkins job what spec file to run. In this case, it
    is going to run the spec file located in the github repo at
    `jobs/validate/logging-spec.yml`.

    We can leave the **@daily** trigger on the jenkins job as our spec file can
    handle determing when to run certain [aspects of the job](https://ogc.8op.org/spec/#plan-specification).

    ### OGC Spec (logging-spec.yml)

    The OGC Spec is how you define what to deploy and test against. There are
    several existing [spec
    files](https://github.com/charmed-kubernetes/jenkins/blob/master/jobs/validate/nfs-spec.yml)
    that could be copied as reference.

    Most spec files will contain a plan, that list different jobs in that plan to test different aspects of your addon.

    In this particular case, let's say we want to test against the following parameters:

      - K8S Snap versions of **1.16/stable** and **1.17/edge**
      - A custom bundle that includes Charmed Kubernetes plus some additional charms
      - A custom integration test
      - Collects some artifacts/data for reporting

    The plan would look something like this:

    ```yaml

    plan:
        - &BASE_JOB
          env:
            - SNAP_VERSION=1.17/edge
            - JUJU_CLOUD=aws/us-east-1
            - JUJU_CONTROLLER=validate-ck-logging
            - JUJU_MODEL=validate-logging-model
            - INTEGRATION_TEST_PATH=jobs/validate
          if: '[[ $(date +"%A") != "Sunday" ]] && [[ $(date +"%A") != "Saturday" ]]'
          before-script:
            - runner:
                timeout: 7200
                script: |
                  #!/bin/bash
                  set -x
                  if ! juju destroy-controller -y --destroy-all-models --destroy-storage $JUJU_CONTROLLER 2>&1; then
                     juju kill-controller -y $JUJU_CONTROLLER 2>&1
                  fi
                  juju deploy -m $JUJU_CONTROLLER:$JUJU_MODEL logging-bundle.yaml 2>&1
                  juju-wait -e $JUJU_CONTROLLER:$JUJU_MODEL -w 2>&1
                assets:
                  - name: logging bundle
                    destination: logging-bundle.yaml
                    source-blob:
                      series: bionic
                      description: A highly-available, production-grade Kubernetes cluster.
                      applications:
                        containerd:
                          annotations:
                            gui-x: '475'
                            gui-y: '800'
                          charm: cs:~containers/containerd-33
                          resources: {}
                        easyrsa:
                          annotations:
                            gui-x: '90'
                            gui-y: '420'
                          charm: cs:~containers/easyrsa-278
                          constraints: root-disk=8G
                          num_units: 1
                          resources:
                            easyrsa: 5

                        ... cut ...

                        kubernetes-worker:
                          annotations:
                            gui-x: '90'
                            gui-y: '850'
                          charm: cs:~containers/kubernetes-worker-590
                          constraints: cores=4 mem=4G root-disk=16G
                          expose: true
                          num_units: 3
                          options:
                            channel: 1.16/stable
                          resources:
                            cni-amd64: 455
                            cni-arm64: 446
                            cni-s390x: 458
                            core: 0
                            kube-proxy: 0
                            kubectl: 0
                            kubelet: 0
                      relations:
                      - - kubeapi-load-balancer:certificates
                        - easyrsa:client
                      - - canal:etcd
                        - etcd:db
                      - - canal:cni
                        - kubernetes-master:cni
                      - - canal:cni
                        - kubernetes-worker:cni
                      - - containerd:containerd
                        - kubernetes-worker:container-runtime
                      - - containerd:containerd
                        - kubernetes-master:container-runtime
          script:
            - runner:
                timeout: 7200
                script: |
                  #!/bin/bash
                  pytest $INTEGRATION_TEST_PATH/test_logging.py::test_logging \
                     --cloud $JUJU_CLOUD \
                     --model $JUJU_MODEL \
                     --controller $JUJU_CONTROLLER

          after-script:
          - runner:
              timeout: 7200
              script: |
                #!/bin/bash
                set -x
                wget https://raw.githubusercontent.com/juju-solutions/cdk-field-agent/master/collect.py
                python3 collect.py -m $JUJU_CONTROLLER:$JUJU_MODEL
                python3 jobs/infra/collect-debug.py push 'cdk_field_agent' results*.tar.gz
                python3 jobs/infra/collect-debug.py push 'build_log' ogc.log
                python3 jobs/infra/collect-debug.py push 'metadata' metadata.json
                python3 jobs/infra/collect-debug.py push 'job_result' *job.json
                python3 jobs/infra/collect-debug.py set-key 'snap_version' "$SNAP_VERSION"
                python3 jobs/infra/collect-debug.py set-key 'juju_deploy_channel' "$JUJU_DEPLOY_CHANNEL"
                python3 jobs/infra/collect-debug.py save-meta
        - <<: *BASE_JOB
          env:
            - SNAP_VERSION=1.16/edge
            - JUJU_CLOUD=aws/us-east-1
            - JUJU_CONTROLLER=validate-ck-logging
            - JUJU_MODEL=validate-logging-model
            - INTEGRATION_TEST_PATH=jobs/validate
    ```


    ### Custom integration test (test_logging.py)

    In the **script** section above, we will be running `pytest` against a suite
    of tests that are typically defined in **jobs/integration/** in the github
    repo.
